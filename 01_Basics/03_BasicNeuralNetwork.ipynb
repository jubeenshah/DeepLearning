{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network for beginners\n",
    "\n",
    "#### Â© Jubeen Shah 2018\n",
    "\n",
    "This jupyter notebook is to get anyone started with the basics of neural networks, and eventually deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importing the numpy library for math operations\n",
    "\n",
    "Click  [Here](http://www.numpy.org/) for more details on NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Defining the sigmoid function\n",
    "`Pseudocode`<code>\n",
    "    \n",
    "    def sigmoid(a_variable, derivative = False):\n",
    "        check if derivative is passed as TRUE as a parameter:\n",
    "        if so:\n",
    "            return x * (1 -x)\n",
    "        otherwise for all other cases:\n",
    "            return 1/(1 + exp(-x))   \n",
    "</code>\n",
    "The formula for sigmoid is given as : \n",
    "$$Sigmoid(x)=\\frac{1}{1 + e^-x} $$\n",
    "\n",
    "The derivative of sigmoid $$\\frac {d}{dx}\\frac{1}{1 + e^-x} = x - x^2 = (x)(1-x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv = False):\n",
    "    if deriv == True:\n",
    "        return x * ( 1 - x )\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Defining the input and the output dataset for the simple neural network\n",
    "\n",
    "|A|B|C|O/P|\n",
    "|---------|\n",
    "|0|0|1|  0|\n",
    "|0|1|1|  0|\n",
    "|1|0|1|  1|\n",
    "|1|1|1|  1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Defining the layers\n",
    "This is our weight matrix for this neural network. It's called \"syn0\" to imply \"synapse zero\". Since we only have 2 layers (input and output), we only need one matrix of weights to connect them. Its dimension is (3,1) because we have 3 inputs and 1 output. Another way of looking at it is that l0 is of size 3 and l1 is of size 1. Thus, we want to connect every node in l0 to every node in l1, which requires a matrix of dimensionality (3,1). :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Forward and backpropogation\n",
    "\n",
    "Uses the \"confidence weighted error\" from l2 to establish an error for l1. To do this, it simply sends the error across the weights from l2 to l1. This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate is : 0.468534325458\n",
      "Error rate is : 0.167929000433\n",
      "Error rate is : 0.165670789246\n",
      "Error rate is : 0.164686021751\n",
      "Error rate is : 0.164093299619\n",
      "Error rate is : 0.16368345298\n",
      "Error rate is : 0.163376950548\n",
      "Error rate is : 0.163135815501\n",
      "Error rate is : 0.162939238195\n",
      "Error rate is : 0.162774703074\n",
      "Error rate is : 0.162634156709\n",
      "Error rate is : 0.162512143433\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(60000):\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0,syn0))\n",
    "    l2 = sigmoid(np.dot(l1,syn1))\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if iteration %5000 == 0:\n",
    "        print(\"Error rate is : \" + str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    l2_delta = l2_error * sigmoid(l2, True)\n",
    "    \n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    l1_delta = l1_error * sigmoid(l1, True)\n",
    "    \n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l2.T.dot(l1_delta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
